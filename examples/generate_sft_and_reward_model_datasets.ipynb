{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Generating SFT and Reward Model Datasets\n",
    "\n",
    "Here we walk through how to generate all intermediate datasets used to train the LC SFT, LC RL, Factuality SFT, and Factuality RL methods in the paper.\n",
    "We provide cached SFT and reward model datasets for all methods and baselines at <https://huggingface.co/datasets/tatsu-lab/linguistic_calibration>. \n",
    "However, if you want to generate the datasets yourself---for example, to use a more sophisticated Summarize() function during summary distillation, or to replicate the pipeline with a stronger base model---you can follow the steps below.\n",
    "\n",
    "### Note on Converting QA Pairs to Decision Tasks\n",
    "As described in the Methods section of the paper (Section 3), we have pre-converted all questions $x$ from off-the-shelf QA datasets into open-ended queries $q$, which prompt the model for open-ended paragraph generations.\n",
    "If you want to convert another QA dataset, use the prompt at `linguistic_calibration/prompts/generating_open_ended_queries/generate_open_ended_query_claude_10shot.txt`.\n",
    "\n",
    "### Generating LC SFT Training Data\n",
    "\n",
    "Our LC SFT model is finetuned using the summary distillation algorithm. Specifically, we use the following procedure:\n",
    "1. For each example in the SFT dataset, sample M long-form paragraph generations from a base model. In our experiments, we use a Llama 2 7B 8-Shot ICL base model and M=8. \n",
    "2. For each example, summarize the M long-form paragraph generations into a single consensus paragraph using an API-based LLM (here, Claude 2.0).\n",
    "3. Construct a dataset of (query, summary paragraph) pairs and finetune the base model (Llama 2 7B Base) on them. \n",
    "\n",
    "This pipeline can also be used to generate the Factuality SFT training dataset, by changing the paragraph generation prompt type and model in step 1, and by removing step 2.\n",
    "Specifically, in step 1, Factuality SFT uses:\n",
    "- model_name=\"llama-2-7b-hf\"\n",
    "- paragraph_generation_prompt_type=\"generate_paragraphs_llama_trivia_qa_icl_8shot\"\n",
    "This is the same as summary distillation, but we only need a single paragraph generation per example which will be specified in a `decoding_args` object."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f90f3b2d91418c49"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cd .."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bd8ab7a55a22ac8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, you should make sure you have set your OpenAI and Anthropic API keys.\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=sk-...\n",
    "export ANTHROPIC_API_KEY=sk-...\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcb743e613c29e87"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. For each example in the SFT dataset, sample M long-form paragraph generations from a base model. In our experiments, we use a Llama 2 7B 8-Shot ICL base model and M=8."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd290e1f9c403ecd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/nband/models/llama-2-7b-hf'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure that you have specified your ICL model base path in the constants.py file.\n",
    "# By default we prompt Llama 2 7B Base with 8-shot ICL, but it is straightforward to extend this pipeline to a base model of choice.\n",
    "\n",
    "from linguistic_calibration import constants\n",
    "\n",
    "ICL_BASE_PATH = constants.SHORT_NAME_TO_MODEL_PATH.get('llama-2-7b-hf'); ICL_BASE_PATH"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T03:18:20.547226Z",
     "start_time": "2024-04-25T03:18:20.534854Z"
    }
   },
   "id": "42ee9d92d9f0cb3b",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# To test this pipeline out, feel free to set MAX_EXAMPLES to a small number.\n",
    "# If you want to generate the full size SFT and reward model datasets, set MAX_EXAMPLES to None.\n",
    "MAX_EXAMPLES = 10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T03:18:20.819847Z",
     "start_time": "2024-04-25T03:18:20.774885Z"
    }
   },
   "id": "322519d4b2998bbb",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "from linguistic_calibration.inference.decode import HFDecodingArguments, decode_prompts_with_model\n",
    "from linguistic_calibration.auto_annotations.qa_auto_eval_utils import format_paragraph_generation_prompt\n",
    "\n",
    "\n",
    "# Load SFT prompts\n",
    "train_dataset = datasets.load_dataset(\n",
    "    constants.HF_DATASETS_PATH,\n",
    "    name=\"sft_training\",\n",
    "    split=\"train\",\n",
    "    cache_dir=constants.DEFAULT_CACHE_DIR,\n",
    ")\n",
    "train_dataset_df = pd.DataFrame(train_dataset)\n",
    "\n",
    "if MAX_EXAMPLES is not None:\n",
    "    train_dataset_df = train_dataset_df.head(MAX_EXAMPLES)\n",
    "    print(f\"Using only {MAX_EXAMPLES} examples for SFT dataset.\")\n",
    "    \n",
    "# These are also the same prompts you would use to generate the Factuality SFT training dataset\n",
    "sft_prompts = format_paragraph_generation_prompt(\n",
    "    train_dataset_df,\n",
    "    model_name=\"llama-2-7b-hf\",\n",
    "    paragraph_generation_prompt_type=\"generate_paragraphs_llama_trivia_qa_icl_8shot\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7030b4354aaff04",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# By default, we sample 8 long-form paragraph generations per example, with temperature 0.7\n",
    "# For Factuality SFT, you should just use num_return_sequences=1\n",
    "generate_multi_sample_decoding_args = HFDecodingArguments(\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=512,\n",
    "    num_return_sequences=8\n",
    ")\n",
    "lc_sft_multisample_generations = decode_prompts_with_model(\n",
    "        prompts=sft_prompts,\n",
    "        model_name=ICL_BASE_PATH,\n",
    "        decoding_args=generate_multi_sample_decoding_args,\n",
    "        per_device_batch_size=1,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5a8eb395a98e45c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['Steven Spielberg directed the 2002 film \"Minority Report,\" which is set primarily in the year 2054. Based on a short story by Philip K. Dick, the film follows a group of people known as \"pre-cogs,\" who can foresee crimes before they happen. The pre-cogs are used by the police to stop crimes before they occur, but the system soon begins to unravel, leading to a dangerous chase between the pre-cogs and the police.',\n 'Steven Spielberg directed the 2002 film \"Minority Report,\" which is set primarily in the year 2054. The film is based on the short story \"The Minority Report\" by Philip K. Dick and stars Tom Cruise, Colin Farrell, and Max von Sydow. \"Minority Report\" tells the story of a futuristic society where individuals with psychic abilities are used to predict and prevent crimes before they happen. However, the use of these psychics raises ethical and legal questions, leading to a conflict between the police and the psychics themselves. The film received positive reviews for its action sequences, special effects, and performances, and was a commercial success, grossing over $219 million worldwide.',\n \"The director of the 2002 film 'Minority Report,' which is set primarily in the year 2054, is Steven Spielberg. The film follows a team of 'Precrime' officers who use a psychic ability to predict and prevent murders before they happen. The movie is set in Washington, D.C., and features a diverse cast of characters, including Tom Cruise as the lead detective John Anderton, Colin Farrell as his partner Danny Witwer, and Max von Sydow as the head of Precrime. 'Minority Report' was a critical and commercial success, grossing over $359 million worldwide and winning several awards, including the Academy Award for Best Visual Effects. The film's innovative use of special effects and its exploration of themes such as privacy, surveillance, and the dangers of predictive technology have made it a classic in the science fiction genre.\",\n 'The director of the 2002 film \"Minority Report,\" which is set primarily in the year 2054, is Steven Spielberg. Spielberg is a renowned American filmmaker known for his work in various genres, including science fiction, action, and drama. \"Minority Report,\" which was based on a short story by Philip K. Dick, is a dystopian thriller that follows the story of a man accused of murder and the police department that uses precognition to prevent crimes before they happen. The film was a critical and commercial success, grossing over $359 million worldwide and receiving numerous accolades, including a nomination for Best Director at the Academy Awards.',\n 'The director of the 2002 film \"Minority Report,\" which is set primarily in the year 2054, was Steven Spielberg. The film is based on a short story by Philip K. Dick and follows the story of a police officer who is part of a special unit that uses psychics to prevent future crimes before they happen. The film stars Tom Cruise, Colin Farrell, and Max von Sydow and was nominated for three Academy Awards. \"Minority Report\" is considered one of Spielberg\\'s most ambitious films and is a classic example of the director\\'s ability to blend science fiction with social commentary.',\n 'Steven Spielberg, an American film director, producer, and screenwriter, helmed the 2002 film \"Minority Report,\" which is set primarily in the year 2054. Based on a short story by Philip K. Dick, the film follows the story of three \"Precogs\" who can predict crimes before they happen and are imprisoned for their ability. Spielberg\\'s visionary direction and attention to detail have made \"Minority Report\" a landmark in science fiction filmmaking, featuring innovative special effects, a compelling storyline, and memorable performances from the cast. The film\\'s critical and commercial success has solidified Spielberg\\'s reputation as one of the most influential and innovative filmmakers of our time.',\n 'Steven Spielberg, an American film director, screenwriter, and producer, helmed the 2002 science fiction thriller \"Minority Report,\" which is set primarily in the year 2054. The film stars Tom Cruise, Colin Farrell, and Max von Sydow, and is based on a short story by Philip K. Dick. The film follows the story of a group of psychics known as \"precogs,\" who are used to predict future murders before they occur. However, the precogs are arrested and imprisoned for their premonitions, leading to a dystopian society where the police can arrest people for crimes they haven\\'t committed yet. \"Minority Report\" was a critical and commercial success, grossing over $359 million worldwide and garnering several awards, including an Academy Award for Best Visual Effects.',\n 'The director of the 2002 film \"Minority Report,\" which is set primarily in the year 2054, is Steven Spielberg. The film is based on a short story by Philip K. Dick, and it explores the concept of a futuristic society where police officers use psychics to prevent crimes before they happen. The film stars Tom Cruise as John Anderton, a police officer who is accused of committing a murder he didn\\'t commit. The film was met with critical acclaim and was nominated for several awards, including Best Director and Best Picture at the Academy Awards. Spielberg\\'s direction and the film\\'s visual effects helped to bring the dystopian future to life, making it a memorable film that continues to be discussed and analyzed today.']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our output list is shape (N, M), where\n",
    "# N is the number of examples in the SFT dataset, and\n",
    "# M is the number of paragraph generations per example.\n",
    "\n",
    "# Since we sample from an ICL model, we postprocess generations to split on the first \"\\n\\n\" token. \n",
    "\n",
    "new_paragraph_generations = []\n",
    "for paragraph_generations in lc_sft_multisample_generations:\n",
    "    new_paragraph_generations.append([generation.strip().split(\"\\n\\n\")[0] for generation in paragraph_generations])\n",
    "    \n",
    "new_paragraph_generations[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T03:58:40.608241Z",
     "start_time": "2024-04-25T03:58:40.600656Z"
    }
   },
   "id": "5a856721a4a8f334",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. For each example, summarize the M long-form paragraph generations into a single consensus paragraph using an API-based LLM (here, Claude 2.0)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61a71181f5a2ae99"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from linguistic_calibration.types import List\n",
    "from linguistic_calibration.utils import read\n",
    "\n",
    "LC_SUMMARIZATION_PREFIX = \"### Thought {idx}\"\n",
    "SUMMARIZATION_PROMPT_TEMPLATE = read(\"src/linguistic_calibration/prompts/paragraph_generation/summarize_claude.txt\")\n",
    "\n",
    "def format_summarization_example(\n",
    "    prompt_template: str, \n",
    "    list_of_M_samples: List[str]\n",
    "):\n",
    "    formatted_paragraphs = \"\"\n",
    "    for i, generated_paragraph in enumerate(list_of_M_samples):\n",
    "        prefix = LC_SUMMARIZATION_PREFIX.format(idx=i+1)\n",
    "        formatted_paragraphs += prefix + \"\\n\" + generated_paragraph + \"\\n\\n\"\n",
    "\n",
    "    return prompt_template.format(formatted_paragraphs=formatted_paragraphs)\n",
    "\n",
    "\n",
    "lc_sft_summarization_prompts = [\n",
    "    format_summarization_example(SUMMARIZATION_PROMPT_TEMPLATE, paragraph_generations)\n",
    "    for paragraph_generations in new_paragraph_generations\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T03:58:42.731189Z",
     "start_time": "2024-04-25T03:58:42.703925Z"
    }
   },
   "id": "87bd515dd6dea712",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Run summarization with Claude 2.0\n",
    "\n",
    "from linguistic_calibration.openai_utils import OpenAIDecodingArguments\n",
    "from linguistic_calibration.inference.decode import get_text_from_completions\n",
    "\n",
    "claude_summarization_decoding_args = OpenAIDecodingArguments(temperature=0.3)\n",
    "\n",
    "lc_sft_summarizations = decode_prompts_with_model(\n",
    "    prompts=lc_sft_summarization_prompts,\n",
    "    model_name=\"claude-2.0\",\n",
    "    decoding_args=claude_summarization_decoding_args,\n",
    ")\n",
    "lc_sft_summarizations = get_text_from_completions(lc_sft_summarizations)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3885eb3e2a7e587",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Construct a dataset of (query, summary paragraph) pairs and finetune the base model (Llama 2 7B Base) on them. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a384be94924d090"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['question_id', 'paragraph_generation_prompt', 'claude_summary'],\n    num_rows: 10\n})"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sft_dataset = pd.DataFrame(\n",
    "    {\n",
    "        \"question_id\": train_dataset_df[\"question_id\"],\n",
    "        \"paragraph_generation_prompt\": train_dataset_df[\"paragraph_generation_prompt\"],\n",
    "        \"claude_summary\": lc_sft_summarizations\n",
    "    }\n",
    ")\n",
    "\n",
    "new_sft_hf_dataset = datasets.Dataset.from_pandas(new_sft_dataset); new_sft_hf_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T04:00:27.472111Z",
     "start_time": "2024-04-25T04:00:27.435177Z"
    }
   },
   "id": "d93a8970b486eabf",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "new_sft_hf_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "535b63d4f04dba41"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Finetuning on the LC SFT Dataset*\n",
    "\n",
    "Now, you can finetune your base model (in our paper, Llama 2 7B Base) on this dataset.\n",
    "Specifically, you could upload this dataset to HF datasets hub or save it locally, and then load it in the `supervised.py` training script by altering the loader method `linguistic_calibration.data_utils.make_linguistic_calibration_supervised_data_module`.\n",
    "\n",
    "See README.md in the main directory for an example of running the LC SFT training script."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aff592d38a92781e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating LC Reward Model Training Data\n",
    "\n",
    "LC RL is trained using decision-based RL. In our instantiation of decision-based RL, we decompose surrogate forecasting into two operations: `ExtractAnswers` and `ForecastProbs` (for more information, refer to the paper).\n",
    "    \n",
    "Following Algorithm 1, we need to:\n",
    "1. Use the LC SFT model to generate paragraphs using the prompts from the Reward Model split of TriviaQA,\n",
    "2. and then use an API-based LLM (Claude 2.0 in our case) to generate answer extractions and probability forecasts.\n",
    "\n",
    "This can be done straightforwardly in a single function call by using the QA auto-evaluation pipeline on the reward model dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a536235ac4223d7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from examples.qa_automated_eval import main as qa_auto_eval_main\n",
    "\n",
    "# Can set however you like\n",
    "REWARD_MODEL_DATASET_OUTPUT_PATH = constants.DEFAULT_OUTPUT_DIR\n",
    "\n",
    "# We assume that you have already finetuned the LC SFT model on the SFT dataset, \n",
    "# and specified its path in the constants.py file with key \"lc_sft\" in the dict constants.SHORT_NAME_TO_MODEL_PATH.\n",
    "LC_SFT_MODEL_NAME = \"lc_sft\"\n",
    "\n",
    "# We specify the prompts for answer extraction and forecasting used in the paper.\n",
    "# We specify \"failout\" for the semantic equivalence prompt, which will intentionally end the script after probability forecasting. You should expect a FileNotFoundError.\n",
    "\n",
    "qa_auto_eval_main(\n",
    "    paragraph_generator_model_name=LC_SFT_MODEL_NAME,\n",
    "    paragraph_generation_prompt=\"generate_paragraphs_llama_finetuned\",\n",
    "    answer_extractor_model_name=\"claude-2.0\",\n",
    "    answer_extractor_prompt=\"train/extract_answers_claude_8shot\",\n",
    "    forecast_probs_model_name=\"claude-2.0\",\n",
    "    forecast_probs_prompt=\"train/forecast_probs_claude_0shot\",\n",
    "    semantic_equivalence_prompt=\"failout\",\n",
    "    dataset_name=\"trivia_qa\",\n",
    "    dataset_split=\"reward_model\",\n",
    "    max_n_examples=MAX_EXAMPLES,\n",
    "    generation_temperature=0.7,  # The same temp we use when sampling during PPO\n",
    "    output_root_dir=REWARD_MODEL_DATASET_OUTPUT_PATH,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72132904b0327eb3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we need to process the outputs from the QA auto-eval pipeline into a format that the reward_modeling.py script accepts.\n",
    "Specifically, we need to use the format in the `reward_model_training` subset of the `tatsu-lab/linguistic_calibration` dataset on HuggingFace.\n",
    "\n",
    "We need the following columns in order to train `ExtractAnswers` and `ForecastProbs`:\n",
    "* question_id: str\n",
    "* lc_sft_generated_paragraph: str\n",
    "* lc_sft_ground_truth_and_extracted_answers: List[str], where the first entry is the ground-truth answer and the remaining are answers extracted with the API-based LLM.\n",
    "* lc_sft_forecasted_probs: List[float], where the first entry is the API-based LLM--forecasted prob for the ground-truth answer and the rest for the extracted answers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c67c7bd5bbe0e829"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "FORECAST_PROBS_RESULTS_PATH = f\"{REWARD_MODEL_DATASET_OUTPUT_PATH}/forecast_probs/trivia_qa/reward_model/lc_sft/claude-2.0/claude-2.0/skip_answer_extraction-False--max_ex-{MAX_EXAMPLES}--seed-42/gen_prompt-generate_paragraphs_llama_finetuned/extr_prompt-train__extract_answers_claude_8shot/forecast_prompt-train__forecast_probs_claude_0shot/gen_temp-0.7/ext_temp-0.2/forecast_temp-0.2/probability_forecasts.csv\"\n",
    "\n",
    "forecast_probs_df = pd.read_csv(FORECAST_PROBS_RESULTS_PATH)\n",
    "\n",
    "reward_model_question_ids = []\n",
    "reward_model_generated_paragraphs = []\n",
    "reward_model_question_id_to_answers = defaultdict(list)\n",
    "reward_model_question_id_to_forecasts = defaultdict(list)\n",
    "\n",
    "for _, row in forecast_probs_df.iterrows():\n",
    "    question_id = row[\"question_id\"]\n",
    "    \n",
    "    if question_id not in reward_model_question_ids:\n",
    "        reward_model_question_ids.append(question_id)\n",
    "        reward_model_generated_paragraphs.append(row[\"generated_paragraph\"])\n",
    "    \n",
    "    reward_model_question_id_to_answers[question_id].append(row[\"ground_truth_top_answer\"])\n",
    "    reward_model_question_id_to_forecasts[question_id].append(row[\"interpretation__forecast_probs\"])\n",
    "    \n",
    "reward_model_dataset = pd.DataFrame(\n",
    "    {\n",
    "        \"question_id\": reward_model_question_ids,\n",
    "        \"lc_sft_generated_paragraph\": reward_model_generated_paragraphs,\n",
    "        \"lc_sft_ground_truth_and_extracted_answers\": [reward_model_question_id_to_answers[question_id] for question_id in reward_model_question_ids],\n",
    "        \"lc_sft_forecasted_probs\": [reward_model_question_id_to_forecasts[question_id] for question_id in reward_model_question_ids]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1f2a443f2935199",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Finetuning ExtractAnswers and ForecastProbs on the RM Dataset*\n",
    "\n",
    "Now, you can finetune the two reward models used in the LC RL pipeline: `ExtractAnswers` and `InterpretProbs`.\n",
    "\n",
    "Specifically, you could upload this dataset to HF datasets hub or save it locally. \n",
    "Then you can:\n",
    "1. Load it during ExtractAnswers training (using the `supervised.py` training script) by altering the loader method `linguistic_calibration.data_utils.make_linguistic_calibration_supervised_data_module`.\n",
    "2. Load it during ForecastProbs training (using the `reward_modeling.py` training script) by altering the loader method `linguistic_calibration.data_utils.make_linguistic_calibration_reward_modeling_data_module`.\n",
    "\n",
    "See README.md in the main directory for a walkthrough of training the ExtractAnswers and ForecastProbs functions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b68b4645f659c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Factuality Binary Correctness Reward Model Dataset\n",
    "\n",
    "You can follow an almost identical approach to generate the Factuality Binary Correctness Reward Model dataset:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd4ffa50e2350a3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from examples.qa_automated_eval import main as qa_auto_eval_main\n",
    "\n",
    "# Can set however you like\n",
    "REWARD_MODEL_DATASET_OUTPUT_PATH = constants.DEFAULT_OUTPUT_DIR\n",
    "\n",
    "# We assume that you have already finetuned the Factuality SFT model on the Factuality SFT dataset, \n",
    "# and specified its path in the constants.py file with key \"factuality_sft\" in the dict constants.SHORT_NAME_TO_MODEL_PATH.\n",
    "FACTUALITY_SFT_MODEL_NAME = \"factuality_sft\"\n",
    "\n",
    "# We specify the prompts for binary correctness annotation used in the paper.\n",
    "# We specify \"failout\" for the semantic equivalence prompt, which will intentionally end the script after binary correctness annotation. You should expect a FileNotFoundError.\n",
    "\n",
    "qa_auto_eval_main(\n",
    "    paragraph_generator_model_name=FACTUALITY_SFT_MODEL_NAME,\n",
    "    paragraph_generation_prompt=\"generate_paragraphs_llama_finetuned\",\n",
    "    forecast_probs_model_name=\"claude-2.0\",\n",
    "    forecast_probs_prompt=\"train/score_binary_correctness_claude_0shot\",\n",
    "    semantic_equivalence_prompt=\"failout\",\n",
    "    skip_answer_extraction=True,\n",
    "    skip_forecast_probs=False,\n",
    "    dataset_name=\"trivia_qa\",\n",
    "    dataset_split=\"reward_model\",\n",
    "    max_n_examples=MAX_EXAMPLES,\n",
    "    generation_temperature=0.7,  # The same temp we use when sampling during PPO\n",
    "    output_root_dir=REWARD_MODEL_DATASET_OUTPUT_PATH,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T08:43:49.442570Z"
    }
   },
   "id": "6e6fd7617f6190ec",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Once again, we process the results from the QA auto-eval pipeline into a format that the reward_modeling.py script accepts, this time for the Factuality Binary Correctness Reward Model dataset.\n",
    "\n",
    "BINARY_CORRECTNESS_RM_RESULTS_PATH = f\"{REWARD_MODEL_DATASET_OUTPUT_PATH}/forecast_probs/trivia_qa/reward_model/factuality_sft/claude-2.0/claude-2.0/skip_answer_extraction-True--max_ex-{MAX_EXAMPLES}--seed-42/gen_prompt-generate_paragraphs_llama_finetuned/extr_prompt-eval__extract_answers_claude_10shot/forecast_prompt-train__score_binary_correctness_claude_0shot/gen_temp-0.7/ext_temp-0.2/forecast_temp-0.2/probability_forecasts.csv\"\n",
    "\n",
    "binary_correctness_rm_df = pd.read_csv(BINARY_CORRECTNESS_RM_RESULTS_PATH)\n",
    "binary_correctness_rm_df = pd.DataFrame(\n",
    "    {\n",
    "        \"question_id\": binary_correctness_rm_df[\"question_id\"],\n",
    "        \"factuality_sft_generated_paragraph\": binary_correctness_rm_df[\"generated_paragraph\"],\n",
    "        \"factuality_sft_binary_correctness\": binary_correctness_rm_df[\"interpretation__forecast_probs\"]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T08:43:49.442854Z"
    }
   },
   "id": "8541e3facb193adf",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Finetuning Factuality RM on the Binary Correctness RM Dataset*\n",
    "\n",
    "Now, you can finetune the reward model used in the training of Factuality RL.\n",
    "\n",
    "Specifically, you could upload this dataset to HF datasets hub or save it locally. \n",
    "Then you can load it during factuality reward modeling (using the `reward_modeling.py` training script) by altering the loader method `linguistic_calibration.data_utils.make_linguistic_calibration_reward_modeling_data_module`.\n",
    "\n",
    "See README.md in the main directory for a walkthrough of training the Factuality RM."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a317d01e4cac367"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
